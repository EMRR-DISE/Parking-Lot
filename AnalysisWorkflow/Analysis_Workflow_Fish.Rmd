---
title: "Analysis_Workflow_Fish"
author: "Catarina Pien"
date: "3/25/2020"
output: 
  html_document:
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preparation {.tabset}
### Load Packages

```{r load, echo = FALSE, results = FALSE, warning = FALSE, message = FALSE}

rm(list=ls(all=TRUE))
if(!require(tidyverse)){install.packages("tidyverse")
    library(tidyverse)}
if(!require(lubridate)){install.packages("lubridate")
    library(lubridate)} # Datetime
if(!require(GGally)){install.packages("GGally")
    library(GGally)} # Correlation Plots
if(!require(gridExtra)){install.packages("gridExtra")
    library(gridExtra)} # Multiple Plots
if(!require(plotly)){install.packages("plotly")
    library(plotly)} # Multiple Plots
source("HighstatLibV10.R")
```

### Import data
* This is a dataset of fish catch for the North Delta Flow Action. 
* Merge flow data into rest of data
* Define variable structures
* Order action phases
* Regional assignments


```{r import and filter dates, warning = FALSE, message = FALSE, results = FALSE}
Fish_ndfa <- read.csv("Data/FISH_MAN_allIEPsurveys_20200527.csv")
FlowDesignation <- read.csv("Data/FlowDatesDesignations.csv")
Regions <- read.csv("Data/Stations_Fish_NDFA_2020-05-28.csv")

# Look at data
head(Fish_ndfa)
str(Fish_ndfa)

# Add variables
Fish_ndfa$Date <- ymd(Fish_ndfa$Date)
Fish_ndfa$Month <- month(Fish_ndfa$Date)
Fish_ndfa$Day <- day(Fish_ndfa$Date)
Fish_ndfa$Year <- ordered(Fish_ndfa$Year)
FlowDesignation$Year <- ordered(FlowDesignation$Year)
FlowDesignation$PreFlowStart <- mdy(FlowDesignation$PreFlowStart)
FlowDesignation$PreFlowEnd <- mdy(FlowDesignation$PreFlowEnd)
FlowDesignation$PostFlowStart <- mdy(FlowDesignation$PostFlowStart)
FlowDesignation$PostFlowEnd <- mdy(FlowDesignation$PostFlowEnd)

# Merge data from FlowDesignation Table (Water Year Type, Flow days and type)
# Filter only Pre-During-Post Flow Action Data. 
# We have a during action, and then Pre = 30 days before/Post = 30 days after
Fish_all0 <- inner_join(Fish_ndfa,FlowDesignation, by = "Year")
Fish_all1 <- left_join(Fish_all0, Regions)
Fish_all <- Fish_all1 %>%
   mutate(ActionPhase = ifelse(Date > PreFlowStart & Date<PreFlowEnd, "Pre", NA)) %>%
   mutate(ActionPhase = replace(ActionPhase, Date > PreFlowEnd & Date < PostFlowStart, "During")) %>% 
   mutate(ActionPhase = replace(ActionPhase, Date > PostFlowStart & Date < PostFlowEnd, "Post")) %>%
  filter(!is.na(ActionPhase)) %>%
   select(-c(PreFlowStart:PostFlowEnd)) %>%
   arrange(Date, Survey, StationCode, CommonName)


# Order Action Phases
Fish_all$ActionPhase <- as.factor(Fish_all$ActionPhase)
Fish_all$ActionPhase <-  factor(Fish_all$ActionPhase, levels(Fish_all$ActionPhase)[c(3,1,2)])

# Define variable structures
Fish_all$Date <- ymd(Fish_all$Date)
Fish_all$Month <- ordered(Fish_all$Month)
Fish_all$Year <- ordered(Fish_all$Year)
Fish_all$WYType <- as.factor(Fish_all$WYType)
Fish_all$ActionPhase <- as.factor(Fish_all$ActionPhase)
Fish_all$X1 <- NULL

```

## Plotting Functions
```{r Functions, warning = FALSE, message = FALSE}
## FUNCTIONS FOR PLOTTING -------------------------------------------------------------
VisPoint <-  function(data,y) {
    y <- enquo(y)
  data %>%
    ggplot() +
    geom_point(mapping = aes(Date,!! y), size = 2) +
    theme_bw() +
  scale_colour_manual(values = c("coral3", "lightseagreen"))+
  theme_bw() + 
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        axis.line = element_line(colour = "black"),
        plot.title = element_text(hjust=0.5),
        axis.text = element_text(size = 11), 
        axis.text.x = element_text(angle = 90, hjust = 1),
        axis.title = element_text(size = 12),
        legend.text = element_text(size = 11),
        legend.position = "bottom")
} 

# Boxplots by variable of interest
VisBox <-  function(data, x, y) {
    x <- enquo(x)
    y <- enquo(y)
  data %>%
    ggplot() +
    geom_boxplot(mapping = aes(!! x,!! y), fill = "lightseagreen", colour = "lightgray") +
    theme_bw() +
  scale_colour_manual(values = c("coral3", "lightseagreen"))+
  theme_bw() + 
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        axis.line = element_line(colour = "black"),
        plot.title = element_text(hjust=0.5),
        axis.text = element_text(size = 11), 
        axis.text.x = element_text(angle = 90, hjust = 1),
        axis.title = element_text(size = 12),
        legend.text = element_text(size = 11))
} 
#----------------------------------------------------------------------------------------------------
####################################################################################################

```

## Exploration of overall dataset
* Station distribution
* Time span
* Number of fish per survey
* Total counts per survey
* Locations

```{r explore, warning = FALSE, message = FALSE}
library(leaflet)
library(plotly)
# Look at locations

# Define palette
pal <- colorFactor(c("slateblue", "darkseagreen", "orange", "orangered", "hotpink"), domain = c("DJFMP", "Yolo", "Townet", "EDSM", "FMWT"))

leaflet(Fish_all) %>%
  addTiles() %>%
  addCircleMarkers(
    color = ~pal(Survey),
    opacity = 0.5,
    lng = ~Longitude,
    lat = ~Latitude,
    label = ~StationCode) %>%
  addLegend(pal = pal,
            values = ~Survey,
            position = "bottomright")

# Time span 
time <- Fish_all%>%
  group_by(Survey, Year) %>%
  summarize(sum.count = sum(totalCount))
ggplot(time, aes(x = Survey, y = Year, fill = sum.count)) + geom_tile() + theme_minimal()

# Look at datasets
Fishsp <- Fish_all %>%
  group_by(CommonName) %>% summarize(count = n())

Fish_all %>%
  plot_ly(x = ~Survey,
          y = ~totalCount,
          type = "box")

Fishsp %>%
  plot_ly(x = ~CommonName, 
          y = ~count,
          type = "bar")
```

## Datasets by fish type
* Make some lists of datasets you might want to filter from the big dataset
```{r species composition, echo=FALSE}
# Lists
list_native <- c("Sacramento Pikeminnow", "Splittail", "Hitch", "Hardhead", "Sacramento Sucker", "Sacramento Blackfish")
list_smeltish <- c("Wakasagi", "Inland Silverside")
list_smeltpred <- c("Largemouth Bass", "Smallmouth Bass", "Striped Bass", "Spotted Bass")
list_cyprinid <- c("Sacramento Pikeminnow", "Sacramento Splittail", "Hitch", "Fathead Minnow",
                           "Golden Shiner", "Hardhead", "Carp", "Goldfish")
list_pelagic <- c("Threadfin Shad", "American Shad", "Striped Bass")

# Filter datasets
Fish_yolo <- Fish_all %>% filter(Survey == "Yolo")
Seine <-  Fish_all %>% filter(MethodCode == "SEIN" | MethodCode == "BSEIN") 
Fyke <- Fish_all %>% filter(MethodCode == "FKTR")
Tows <- Fish_all %>% filter(Survey %in% c("EDSM", "Townet", "FMWT"))
```

# Seine Data 
## Data Prep{.tabset}
* Filter to just seine data
* Fill in zeros for unlisted species
* Calculate CPUE 
* Organize data the way you want it
```{r seine, warning = FALSE, message = FALSE}

### Complete Cases
# For each Date, Survey, StationCode combination, make sure each fish species is represented with 
# either positive count or zero. 
Seine_completecase <- Seine %>%
  group_by(Date, Survey, StationCode, CommonName) %>%
  summarize(sum.count = sum(totalCount)) %>%
  ungroup() %>%
  complete(CommonName, nesting(Date, Survey, StationCode), fill = list(sum.count = 0)) %>%
  arrange(Date,Survey, StationCode,CommonName)

### Merge back together with rest of data

# Get distinct samples for looking at Water Quality 
Seine_samples <- Seine %>% select(-c(CommonName, totalCount)) %>% distinct()

# Merge 
Seine_complete <- left_join(Seine_completecase, Seine_samples, by = c("Date", "Survey", "StationCode"))
# There is a Yolo sample with two volumes... this is why there are more rows of Seine_complete

### Calculate CPUE
# Remove samples with no volume 
Seine_CPUE <- Seine_complete %>%
  filter(!is.na(VolumeSampled))%>%
  mutate(CPUE = round(sum.count/VolumeSampled,2))

### Rearrange columns 
Seine_f <- Seine_CPUE[, c("Date", "Year", "Month", "Day", "Survey", "StationCode", 
                          "Latitude", "Longitude", "Region", "MethodCode",
                         "WYType", "FlowPulseType", "NetFlowDays","ActionPhase", 
                         "Secchi", "Turbidity", "Conductivity", "WaterTemp", "DO", "Tow",
                         "Depth", "VolumeSampled",  "CommonName", "sum.count", "CPUE")]

### Mean CPUE 
## Calculate means for each species by year-location-actionphase
CPUE_means_seine_phase <- Seine_f %>%
  group_by(Year, Survey, StationCode, ActionPhase, CommonName) %>% 
  summarize(mean.CPUE = mean(CPUE)) 

```

## Data Exploration
### Water Quality
* Simple point plots and histograms, correlation matrix
```{r envexplore, warning = FALSE, message = FALSE, fig.height=11, fig.width=10}
############ OUTLIERS #################
# Boxplots
WTvis1 <- VisPoint(Seine_samples, WaterTemp)
WTvis2 <- VisBox(Seine_samples, Month, WaterTemp)
WTvis3 <- VisBox(Seine_samples, Year, WaterTemp)
Cvis1 <- VisPoint(Seine_samples, Conductivity)
Cvis2 <- VisBox(Seine_samples, Month, Conductivity)
Cvis3 <- VisBox(Seine_samples, Year, Conductivity)
Svis1 <- VisPoint(Seine_samples, Secchi)
Svis2 <- VisBox(Seine_samples, Month, Secchi)
Svis3 <- VisBox(Seine_samples, Year, Secchi)
Tvis1 <- VisPoint(Seine_samples, Turbidity)
Tvis2 <- VisBox(Seine_samples, Month, Turbidity)
Tvis3 <- VisBox(Seine_samples, Year, Turbidity)
DOvis1 <- VisPoint(Seine_samples, DO)
DOvis2 <- VisBox(Seine_samples, Month, DO)
DOvis3 <- VisBox(Seine_samples, Year, DO)

# Plot together
grid.arrange(WTvis1, WTvis2, WTvis3, Cvis1, Cvis2, Cvis3, Svis1, Svis2, Svis3, 
             Tvis1, Tvis2, Tvis3, DOvis1, DOvis2, DOvis3, ncol = 3)

# Boxplots
plot_ly(data = Seine_samples, x = ~StationCode, y = ~WaterTemp, color = ~StationCode, type = 'box')
plot_ly(data = Seine_samples, x = ~StationCode, y = ~Conductivity, color = ~StationCode, type = 'box')
plot_ly(data = Seine_samples, x = ~StationCode, y = ~Secchi, color = ~StationCode, type = 'box')
plot_ly(data = Seine_samples, x = ~StationCode, y = ~Turbidity, color = ~StationCode, type = 'box')
plot_ly(data = Seine_samples, x = ~StationCode, y = ~DO, color = ~StationCode, type = 'box')

############ CORRELATIONS ####################
# Correlation Matrix WQ
Corr.wq <- Seine_samples %>% select(WaterTemp, Conductivity, Secchi, Turbidity, DO)
ggpairs(Corr.wq)

# Variance Inflation Factor (VIF)
corvif(Corr.wq) # Want to get rid of the variable if VIF > 4

```

## Clean up WQ data (QC)
* Edit anything that needs to be removed, flagged, changed.
* Replace Missing Data

```{r clean, warning = FALSE, message = FALSE}
# Clean up wq data
# QC Check - does anything look weird?
Seine_samples %>% filter(WaterTemp>40 | WaterTemp<1)
Seine_samples %>% filter(Secchi > 0.95)
Seine_samples %>% filter(Conductivity > 4000)
Seine_f$WaterTemp[Seine$WaterTemp == 50.6] <- NA

# Function to fill in missing values with mean 
impute.mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))

# Filling in missing values using impute.mean function from above. 
# Variables are renamed because using mutate adds on new columns to the matrix.
# Use impute.mean to fill in NAs, rename updated variables using mutate
Seine_f <- Seine_f %>%
  group_by(WYType) %>%
  mutate(
    Cond = impute.mean(Conductivity),
    WTemp = impute.mean(WaterTemp),
    SecDepth = impute.mean(Secchi),
    Turb = impute.mean(Turbidity),
    DOx = impute.mean(DO)) %>%
  ungroup()
```

# Data Analysis (Seine data) {.tabset}
1. Filter for species or sets of species
```{r Seine filters, warning = FALSE, message = FALSE}

list_nmds <- c("Sacramento Pikeminnow", "Splittail", "Hitch", "Hardhead", "Sacramento Sucker", "Sacramento Blackfish",
               "Wakasagi", "Inland Silverside", "Delta Smelt", 
               "Carp", "Goldfish", "Hardhead", "Golden Shiner", "Fathead Minnow", "Hitch",
               "Rainwater Killifish", "Western Mosquitofish", "Black Crappie", "White Crappie", "Bluegill", "Bigscale Logperch",
               "Largemouth Bass", "Smallmouth Bass", "Striped Bass", "Spotted Bass",
               "Threadfin Shad", "American Shad")
list_nmds_small <- c("Sacramento Pikeminnow", "Splittail", "Hitch", "Hardhead", "Sacramento Sucker", "Sacramento Blackfish",
               "Wakasagi", "Inland Silverside", "Delta Smelt", "Longfin Smelt", 
               "Largemouth Bass", "Smallmouth Bass", "Striped Bass", "Spotted Bass",
               "Threadfin Shad", "American Shad", "Bluegill", "Black Crappie", "Bigscale Logperch")

Seine_tf <- Seine_f %>% filter(CommonName == "Threadfin Shad")
Seine_sucker <- Seine_f %>% filter(CommonName == "Sacramento Sucker")
Seine_lmb <- Seine_f %>% filter(CommonName == "Largemouth Bass")
Seine_black <- Seine_f %>% filter(CommonName == "Sacramento Blackfish")
Seine_pike <- Seine_f %>% filter(CommonName == "Sacramento Pikeminnow")
Seine_natives <- Seine_f %>%filter(CommonName %in% list_native)
Seine_nmds_larger <- Seine_f%>% filter(CommonName %in%list_nmds)
Seine_nmds_0 <- Seine_f %>% filter(CommonName %in% list_nmds_small)
```

## Univariate Analyses {.tabset}
### Parametric Analyses 

#### T-test 
*Data must (approximatley) fit a known statistical model

*Highly used and have a long history

*Measures the difference in population means 

1. Independent sample t-test (two sample t-test or students t-test)
      - determines whether there is a statistically significant difference between the means in         two unrelated groups
      - example: Whether first year graduate salaries differed based on gender 
2. Paired t-test
      - compares two population means where you have two samples in which observations in one           sample can be paired with observations in the other sample.
      - example: Before and after measurments on the same subjects 
      
3. One-sample t-test
      -  used to determine whether a sample comes from a population with a specific mean
      -  example: You sample 1000 doctors and see if their hours differ from 100 hours.
      
```{r ttest, warning = FALSE, message = FALSE}

### 1. Independent-samples t-test: Is there a difference in LMB CPUE by survey? ----------------
(lmb.ttest <- t.test(Seine_lmb$CPUE~Seine_lmb$Survey)) # not significant

# Plot
ggplot(Seine_lmb, aes(x = Survey, y = CPUE)) + geom_boxplot()
ggplot(Seine_lmb, aes(x = CPUE, color = Survey)) + geom_density()
ggplot(Seine_lmb, aes(x = Survey, y = CPUE)) + geom_col() # This works best for data with lots of zeros

### 2. Paired t-test: Does a treatment cause a difference? Did Action Phase alter CPUE of LMB? -------------
Seine_lmb2 <-  filter(Seine_lmb, ActionPhase!="During")
(lmb.ttest2 <- t.test(Seine_lmb2$CPUE ~ Seine_lmb2$ActionPhase)) # significant

# Plot: Figure out the direction of the trend
ggplot(Seine_lmb2, aes(x = ActionPhase, y = CPUE)) + geom_boxplot()
ggplot(Seine_lmb2, aes(x = CPUE, color = ActionPhase)) + geom_density()
ggplot(Seine_lmb2, aes(x = ActionPhase, y = CPUE)) + geom_col() # This works best for data with lots of zeros

### 3. One-sample t-test: Is the CPUE greater than 0? ---------------------------------------
(lmb.ttest3 <- t.test(Seine_lmb$CPUE, mu = 0)) # p < 0.05, Yes, it is

```

#### ANOVA {.tabset}
##### Run ANOVA
1. One-way
    - used to determine whether there are any statistically significant differences between the       means of two or more independent (unrelated) groups (although you tend to only see it used       when there are a minimum of three). 
    
2. Two-way
    - used to compare the mean differences between groups that have been split on two                 independent variables (called factors). The primary purpose of a two-way ANOVA is to            understand if there is an interaction between the two independent variables on the              dependent variable.

```{r anova, warning = FALSE, message = FALSE} 
### 1. One-way ANOVA: Effect of action phase on Threadfin Shad CPUE
(tf.aov1 <- aov(CPUE~ActionPhase, data = Seine_tf))
summary(tf.aov1)

### 2. Two-way ANOVA: Effect of action phase and water year type on Threadfin Shad CPUE
(tf.aov2 <- aov(CPUE~ActionPhase + Region, data = Seine_tf))
summary(tf.aov2)

### If both significant, check interaction.
(tf.aov3 <- aov(CPUE~ActionPhase + Region, data = Seine_tf))
summary(tf.aov3)

```

##### Test Assumptions 
- Plot results, can use statistical tests to check if desired but pretty sensitive.
1. Residuals are normally distributed
2. Residuals are homogeneous across groups
```{r anovaassumptions, message = FALSE, warning = FALSE}
library(car)

# Test for normality of residuals
par(mfrow = c(1,1))
plot(tf.aov2,2)
tf.resid <- residuals(object = tf.aov2)
shapiro.test(tf.resid) # p < 0.05 means not normal

# Test for homogeneity of variance. 
# If data are normal: Bartlett's test
# If data are nonnnormal or Fligner-Killeen Test: Levene's test (In this case, use this one)
plot(tf.aov2, 1)
bartlett.test(CPUE~interaction(ActionPhase,Region), data = Seine_tf) #p<0.05 means they are NOT homogeneous
leveneTest(CPUE~ActionPhase*Region, data = Seine_tf) #p<0.05 means they are NOT homogeneous
```

##### Post-Hoc Tests
P-Value Corrections: 
* Tukey: set family error rate
* Bonferroni: divide p-value by number of comparisons
* Holm-Bonferroni : sequential bonferroni, more powerful than bonferroni
```{r posthoc, warning = FALSE, message = FALSE}
library(lsmeans)
library(multcomp)
library(multcompView)
leastsquare.tf = lsmeans(tf.aov2, 
                         ~ActionPhase|Region,
                         adjust = "holm")

# Compact letter display - test each comparison
(CLD <- cld(leastsquare.tf, alpha = 0.05, Letters = letters, adjust = "holm"))
```


##### Plot Results
* https://rcompanion.org/handbook/G_06.html

```{r plotanova, warning = FALSE, message = FALSE}
library(FSA)

# One plot idea
pd = position_dodge(0.4)    ### How much to jitter the points on the plot

ggplot(CLD,
       aes(x     = Region,
           y     = lsmean,
           color = ActionPhase,
           label = .group)) +

    geom_point(shape  = 15,
               size   = 4,
             position = pd) +

    geom_errorbar(aes(ymin  =  lower.CL,
                      ymax  =  upper.CL),
                      width =  0.2,
                      size  =  0.7,
                      position = pd) +

    theme_bw() +
    theme(axis.title   = element_text(face = "bold"),
          axis.text    = element_text(face = "bold"),
          plot.caption = element_text(hjust = 0)) +

    ylab("Mean CPUE") +
     ggtitle ("Mean Threadfin Shad CPUE",
            subtitle = "By Region and Action Phase") +
 
            labs(caption  = paste0("\nInterpretation ",
                                   "here \n"),
                            hjust=0.5) +
 
  geom_text(nudge_x = c(0.1, -0.1, 0.1, -0.1, 0.1, -0.1, -0.1, 0.1),
            nudge_y = c(0.1,  0.05, 0.05,  0.05, 0.05 , 0.05,  0.05, 0.05),
            color   = "black") +
 
  scale_color_manual(values = c("blue", "red", "green"))


# Graphics
# Using standard error
# Need to manually insert the lsmeans significant groupings

Sum.tf = Summarize(CPUE~ActionPhase + Region, data = Seine_tf, digits = 3)
Sum.tf$se = Sum.tf$sd/sqrt(Sum.tf$n)
Sum.tf$se = signif(Sum.tf$se, digits = 3)

#levels(Sum.tf$Season) <- c("Dry 2015", "Wet", "Dry 2016")

ggplot(Sum.tf, aes(x = Region, y = mean, color = ActionPhase)) +
  geom_errorbar(aes(ymin = mean-se,
                    ymax = mean +se),
                width = 0.2, size = 0.7, position = position_dodge(0.2)) +
  geom_point(aes(shape = ActionPhase), size = 4, position = position_dodge(0.2)) +
  # annotate("text", x = 1, y = 5.5, label = "e", size = 6) +
  # annotate("text", x = 1.15, y = 3.8, label = "de", size = 6) +
  # annotate("text", x = 1.1, y = 2.5, label = "bcd", size = 6) +
  # annotate("text", x = 1.85, y = 0.6, label = "ab", size = 6) +
  # annotate("text", x = 2, y = 1, label = "ab", size = 6) +
  # annotate("text", x = 2.2, y = 0.65, label = "abc", size = 6) +
  # annotate("text", x = 2.9, y = 0.45, label = "a", size = 6) +
  # annotate("text", x = 3.05, y = 0.4,label = "ab", size=6) +
  # annotate("text", x = 3.2, y = 4, label = "cde", size = 6) +
  labs(y = expression(paste("Mean CPUE (ind   ", m^{-3},")")))+
  theme_bw() +
  theme(axis.title = element_text(face = "bold"),
        axis.text = element_text(size = 14),
        legend.text = element_text(size = 13)) +
  scale_colour_manual(values = c("#1db918", "#2e8fad", "#e52078"))
```


#### ANCOVA

*ANCOVA is a blend of analysis of variance (ANOVA) and regression. It is similar to factorial ANOVA, in that it can tell you what additional information you can get by considering one independent variable (factor) at a time, without the influence of the others.

Assumptions: 
* Linearity between covariate and outcome variable at each level of grouping variable (check with grouped scatter plot of covariate and outcome variable)
* Homogeneity of regression slopes 
* Outcome variable should be approximately normally distributed
* Homescedasticity of residuals variance for all groups
* No signficant outliers 

```{r ancova}

ggplot(Seine_tf, aes(x = WTemp, y = CPUE, group = Region, color = Region)) +
  geom_point() +
  stat_smooth(method = "loess", span = 0.9) 

anova_test(CPUE ~ Region * WTemp)
```

### Non-parametric Analyses {.tabset}

#### Mann-Whitney Wilcoxon: compares two groups of continuous data
http://www.r-tutor.com/elementary-statistics/non-parametric-methods/mann-whitney-wilcoxon-test

Is there a difference between Threadfin CPUE for pre and post action phase? 
```{r mannwhitney}
TwoGroups <- filter(Seine_tf, ActionPhase %in% c("Pre", "Post"))
wilcox.test(CPUE ~ ActionPhase, data = TwoGroups)
```

#### Kruskal-Wallis: same as above, but only for one factor
* https://rcompanion.org/rcompanion/d_06.html
```{r kruskalwallis, warning = FALSE, message = FALSE}
library(lattice)
# Check distributions are similar
histogram(~CPUE  | ActionPhase,
          data = Seine_tf,
          layout = c(1,3))
boxplot(CPUE~ActionPhase,
        data = Seine_tf, 
        ylab = "ActionPhase",
        xlab = "CPUE")

kruskal.test(CPUE~ActionPhase, data = Seine_tf)
```

Post-hoc for Kruskal-Wallis
* Dunn Test: Appropriate for groups with unequal numbers of observations (Zar 2010)
* Nemenyi test: Not appropriate for groups with unequal numbers of observations (Zar 2010)
* Pairwise Mann-Whitney U
```{r dunntest}
library(FSA)
library(rcompanion)

### Dunn
(tf.dunn <- dunnTest(CPUE~ActionPhase, 
                    data = Seine_tf,
                    method = "holm"))

# Get the letters wit compact letter display
tf.res <- tf.dunn$res
cldList(comparison = tf.res$Comparison,
        p.value = tf.res$P.adj,
        threshold = 0.05)

```

#### Kolmogorov-Smirnov Test
* Are two distributions different from each other?
* Lower D statistic = more probable that samples came from same distribution. If D >1, more likely they are not from the same distribution
* Can follow up to ask if one is larger than the other
* Example: Compare size distributions from Male vs. Female fish
* Here: Compare CPUE distributions from Threadfin Shad vs. Inland Silverside

```{r kstest, warning = FALSE, message = FALSE}

# ECDF and K-S TEST
TF <- filter(Seine_f, CommonName=="Threadfin Shad")
ISS <- filter(Seine_f, CommonName == "Inland Silverside")
ks.test(TF$CPUE, ISS$CPUE)

# Plot cumulative distribution
# Plot data (CPUE) from least to greatest.
Seine_ecdf <- filter(Seine_f, CommonName %in% c("Threadfin Shad", "Inland Silverside"))
ggplot(Seine_ecdf, aes(CPUE, color=CommonName)) +
  stat_ecdf() +
  labs(x= "CPUE",
       y = "Cumulative Probability",
       title = "ISS and Threadfin Shad") +
  theme_bw() +
  annotate("text", x=100, y=0.7, label="D = 0.47582", size=6) +
  annotate("text", x =100, y = 0.625, label = "p < 0.05", size = 5)

# Visual representation of their distributions. D<1 represents they come from similar distributions.
```


#### Goodness of Fit
### Chi Square
http://www.sthda.com/english/wiki/chi-square-goodness-of-fit-test-in-r
* Compare the observed distribution to an expected distribution in a situation where we have two or more categories in a discrete dataset. Compares multiple observed proportions to expected probabilities. 
* Use raw values 
```{r chisquare}
library(viridis)

# Question: Are LMB equally common in each region?
Counts_lmb <- Seine_lmb %>%
  group_by(Region) %>%
  summarize(Count = sum(sum.count)) %>%
  ungroup()

v_lmb <- as.vector(Counts_lmb$Count)
exp <- rep(0.25,4)
(res <- chisq.test(x = v_lmb, p = exp)) # make sure to have "p=" if you are using proportions

# LMB counts are significantly different by region.

sum(Counts_lmb$Count)
Chi_table_plot <- Counts_lmb %>%
  mutate(Obs.Prop = Count/sum(Count),
         Exp.Prop = .25)
Chi_long <- pivot_longer(Chi_table_plot, -c(Region, Count), names_to = "ObsExp", values_to = "Prob")

ggplot(Chi_long, aes(x = Region, y = Prob, fill = ObsExp) ) + geom_col(position = position_dodge()) + scale_fill_viridis(discrete=TRUE, option = "cividis") + theme(axis.text.x = element_text(angle = 90))
```

### Generalized Linear Model Code
* Used when you want to specify different types of distributions (non-parametric), variance structures, spatial, time structures, etc. 
1. Check out data - what kind of distribution does this look like? 
+ If there are a lot of zeros, you might want to look at zero-inflated possion or negative binomial
2. Scale and center continuous variables if very different
3. Run full model (all variables)
4. Run alternate models, or run dredge to run all possible model combinations
5. Use AIC to pick best model
6. Check model 

```{r glm and dredge}
library(MuMIn)
library(pscl)
library(AER)
library(visreg)
library(countreg)

# Look at distribution of data - is it zero-inflated?
ggplot(Seine_tf, aes(CPUE)) + geom_histogram(binwidth = 0.1) + theme_bw()
plot_ly(data = Seine_tf, x = ~Survey, y = ~CPUE,  type = 'box')

# Scale and center continuous variables if needed (variables have very different units)
Seine_tf_scale <- Seine_tf %>%
  mutate(Turb2 = scale(Turb, center = TRUE),
         Cond2 = scale(Cond, center = TRUE),
         WTemp2 = scale(WTemp),
         DOx2 = scale(DOx))

# For CPUE, use offset to account for sampling effort
Seine_tf_scale$Samp <- log(Seine_tf_scale$VolumeSampled)

# Start with normal poisson
L0 <- glm(sum.count ~ Region + ActionPhase +  WTemp + Cond +
            offset(Samp), family = poisson, data = Seine_tf_scale)

# Run backwards model to eliminate non-significant variables
TF.back <- step(L0, direction = "backward")
summary(TF.back) # Look at results

par(mfrow = c(2,2))
plot(TF.back) # Check residuals
visreg(TF.back) # Predictive plots

# Check for overdispersion. Overdispersed if >1: Use NB
dispersiontest(TF.back, trafo = 1)

### Model: zero-inflated poisson -------------------------------
# You can only have full count data, not CPUE.
# Need to add the offset! 

f1 <- formula(sum.count ~ Region + Survey + ActionPhase +
                Cond + WTemp+ offset(Samp))
zip1 <- zeroinfl(f1, dist = "poisson", link = "logit", data = Seine_tf_scale); summary(zip1)


### Model: zero-inflated negative binomial -------------------------------------
zinb1 <- zeroinfl(f1, na.action = "na.fail", dist="negbin", link = "logit", data = Seine_tf_scale); summary(zinb1)

### Dredge function runs all the available models to tell you what the best model is.
# AIC within 2 = about the same model
# Pick what makes sense for your system, or average your top models

# Use AIC to pick the best models, then look at variables that aren't significant, and re-run model without significant variables
# m1.dredge <- dredge(zinb1)

# Take out survey (not significant)
f2 <- formula(sum.count ~ Region + ActionPhase + Cond2 + WTemp2)
zinb2 <- zeroinfl(f2, na.action = "na.fail", dist="negbin", link = "logit", data = Seine_tf_scale) ; summary(zinb2)

# Look at all variables and the different combo of models, which AIC is lowest.
m2.dredge <- dredge(zinb2)

# Diagnostic Plots -------------------------------------
# Model Validation 
# Calculate residuals
EP1 <- resid(zinb2, type = "pearson")

# Diagnostic Plots
par(mfrow = c(2,3))
plot(x=zinb2$fitted.values, y = EP1, main = "Pearson residuals")
qqrplot(EP1) # Don't know why not working
plot(x=Seine_tf_scale$ActionPhase,y = EP1, main = "Action Phase")
plot(x=Seine_tf_scale$Cond2, EP1, main = "Conductivity")
plot(x=Seine_tf_scale$WTemp2, EP1, main = "Water Temperature")

visreg(zinb2)
```


### General Additive Modeling
https://noamross.github.io/gams-in-r-course/
* Good for non-linear data
```{r GAM}
library(gam)
library(mgcv)
gam1 <- gam(sum.count ~ s(WTemp2) + s(Cond2) + ActionPhase + Region, data = Seine_tf_scale, method = "REML")

# Interpret results
summary(gam1)

# Plot
plot(gam1,  all.terms = TRUE, se = TRUE)
# Significance: you cannot draw a horizontal line through the 95% CI

# Check output
gam.check(gam1)
# Convergence, model residuals should be random, small p-values indicate residuals are not randomly distributed. 
concurvity(gam1, full = TRUE) # If value > 0.8, inspect model more carefully.
concurvity(gam1, full = FALSE)

# More models
# Look at variables by certain factor
gam2 <- gam(sum.count~s(WTemp2, by = Region) + s(Cond2, by = Region) + ActionPhase, data = Seine_tf_scale, method = "REML")
summary(gam2)
plot(gam2, all.terms = TRUE, se = TRUE)
     
     
```







## Multivariate Analyses {.tabset}

### Preparation
1. Some summary stats 

3. Look for outliers in species data
4. Look at initial trends - env. vs. species
5. Pick transformations needed

1. Summary Stats
```{r stats, warning = FALSE, message = FALSE}
snstatsall <- Seine_nmds_larger%>% 
  group_by(CommonName) %>%
  summarize(mean = mean(CPUE), 
            max = max(CPUE),
            min = min(CPUE), 
            sum = sum(CPUE),
            CV = sd(CPUE)/mean,
            zeros = sum(CPUE == 0),
            n = n(), 
            prop.absent = round(zeros/n,2))

snstats <- Seine_nmds_0 %>% 
  group_by(CommonName) %>%
  summarize(mean = mean(CPUE), 
            max = max(CPUE),
            min = min(CPUE), 
            sum = sum(CPUE),
            CV = sd(CPUE)/mean,
            zeros = sum(CPUE == 0),
            n = n(), 
            prop.absent = round(zeros/n,2))
```

2. Remove certain species
* Need to make decisions about which species to keep in
* Species occurring in fewer than 5% samples
* Species occurring in >95% samples
* Species with CV (standard deviation/mean) < 10%

Here: 
* Remove species that are not present: Hardhead, Smallmouth Bass, Longfin Smelt
* Species < 5% of samples: Hitch, Wakasagi, Delta Smelt, Sacramento Blackfish

```{r clean species}
notabund <- c("Hardhead", "Smallmouth Bass", "Longfin Smelt", "Hitch", "Wakasagi", "Delta Smelt", "Sacramento Blackfish")
Seine_nmds <- filter(Seine_nmds_0, !(CommonName %in% notabund))
```

### Exploration/QC
1. Look for outliers in species data
* Click species to remove, look at each individual species for any obvious outliers
```{r sp outliers, warning = FALSE, message = FALSE}
# Overall CPUE
par(mfrow = c(1,1))
index= seq(1,length(Seine_nmds$CPUE))

# Explore each species - look for outliers here by adding and removing species
Seine_nmds%>%
  plot_ly() %>%
  add_trace(x = ~index, 
            y = ~CPUE, 
            color = ~CommonName,
            colors = "Set3",
            type = "scatter")

```


2. Look at initial trends - environment vs. species

```{r trends, warning = FALSE, message = FALSE}
# Threadfin vs environmental
threadfin1 <- ggplot(Seine_tf, aes(x = WTemp, y = CPUE)) + geom_point() + theme_bw()
threadfin2 <- ggplot(Seine_tf, aes(x = Cond, y = CPUE)) + geom_point() + theme_bw()
threadfin3 <- ggplot(Seine_tf, aes(x = Turb, y = CPUE)) + geom_point() + theme_bw()
threadfin4 <- ggplot(Seine_tf, aes(x = DOx, y = CPUE)) + geom_point() + theme_bw()
grid.arrange(threadfin1, threadfin2, threadfin3, threadfin4, top = "Threadfin CPUE by Environmental Variable")

# Sac Sucker vs environmental
sucker1 <- ggplot(Seine_sucker, aes(x = WTemp, y = CPUE)) + geom_point() + theme_bw()
sucker2 <- ggplot(Seine_sucker, aes(x = Cond, y = CPUE)) + geom_point() + theme_bw()
sucker3 <- ggplot(Seine_sucker, aes(x = Turb, y = CPUE)) + geom_point() + theme_bw()
sucker4 <- ggplot(Seine_sucker, aes(x = DOx, y = CPUE)) + geom_point() + theme_bw()
grid.arrange(sucker1, sucker2, sucker3,sucker4, top = "Sac Sucker CPUE by Environmental Variable")

# Largemouth Bass vs environmental
lmb1 <- ggplot(Seine_lmb, aes(x = WTemp, y = CPUE)) + geom_point() + theme_bw()
lmb2 <- ggplot(Seine_lmb, aes(x = Cond, y = CPUE)) + geom_point() + theme_bw()
lmb3 <- ggplot(Seine_lmb, aes(x = Turb, y = CPUE)) + geom_point() + theme_bw()
lmb4 <- ggplot(Seine_lmb, aes(x = DOx, y = CPUE)) + geom_point() + theme_bw()
grid.arrange(lmb1, lmb2, lmb3, lmb4, top = "Largemouth Bass CPUE by Environmental Variable")
```

### Transformations
* For highly skewed variables, and to help you  meet assumptions of a statistical test
* Can help emphasize presence/absence
* Equalize relative importance of variabilities
* Pick transformations that do not change the rank - power, log, arcsine, arcsine sqrt
1. Log of sqrt for highly skewed data, or ranging >2 order magnitude
2. Arcsine sqrt for proportional data
3. Use same transformation for same variable set (e.g. species)
4. Consider binary transformation when percent of zero values is high (>50%) or distinct values low (<10)

```{r transformtests, warning = FALSE, message = FALSE}

# Environmental Matrix
Seine_pca <- sample_n(Seine_nmds, 300)
Seine_env <- Seine_pca %>% dplyr::select(c(26:30))

# Check for normality of variables
cond <- Seine_env$Cond
temp <- Seine_env$WTemp
sd <- Seine_env$SecDepth
turb <- Seine_env$Turb
do <- Seine_env$DOx

hist(cond)
hist(log(cond+1)) # Use this
qqnorm(log(cond+1))

hist(temp)
hist(log(temp+1)) # Use this
hist(sqrt(temp))
qqnorm(temp)
qqnorm(log(temp+1))

hist(sd) # Keep it?
hist(log(sd+1))
qqnorm(sd)
qqnorm(log(sd+1))

hist(turb)
hist(sqrt(turb))
hist(log(turb+1))
qqnorm(log(turb+1)) # Use this

hist(do) # Keep it
qqnorm(do)
```
 
### PCA {.tabset}
#### Run PCA
1.  Apply transformations
2.  If you have variables with different units/scales, choose the correlation matrix (scale = TRUE). Then you do not need to scale your data beforehand.
3. Run PCA
4. Check PCA (Scree plot, Randomization Tests)

* https://cran.r-project.org/web/packages/ggfortify/vignettes/plot_pca.html

```{r pca, warning = FALSE, message = FALSE}
source('biostats.R')
library(vegan)
library(ggfortify)

### Transform to get close to normality assumptions
Seine_env_t <- Seine_env %>% mutate(Cond = log(Cond + 1),
                                    WTemp = log(WTemp + 1),
                                    Turb = log(Turb + 1)) %>%
  as.data.frame()

row.names(Seine_env_t) <- row.names(Seine_env_t)

### Run PCA. Center and Scale the data (correlation matrix).
pca.env <- prcomp(Seine_env_t, center=T, scale.=T)
summary(pca.env)

### Check how well PCA worked
# Scree Plot with Broken Stick values. 
# If eigenvalue is greater than broken stick value it is "significant"
screeplot(pca.env, bstick = TRUE) # According to this, don't keep PC1 and PC2

# Monte Carlo Randomization
# This is slow. Ideally, have several dimensions (up to number of variables) to look at trend.
# ordi.monte(Seine_env_t, ord = 'pca', dim = 5) # According to this, keep PC1 and PC2

### Check which variables matter
# Loadings. Generally, if magnitude >/= 0.3 this is important.
pca.env$rotation
# Structure coefficients: linear correlations between original variables and PC scores
pca.structure(pca.env, Seine_env_t, dim = 5, cutoff = 0.4)
```

#### Plot
5. Plot and interpret PCA

```{r plotPCA, warning = FALSE, message =  FALSE}

# Autoplot has best customization, looks nice
  # use the "data = " to bring in original dataset so we can 
  # color-code by factors
autoplot(pca.env, data = Seine_pca, colour = 'ActionPhase', loadings = TRUE,
         loadings.label = TRUE,
         loadings.colour = "black",
         loadings.label.vjust = -.5,
         loadings.label.col = "black",
         loadings.label.size = 4,
         loadings.label.font = 4) +
  theme_bw()
autoplot(pca.env, data = Seine_pca, colour = 'WYType', loadings = TRUE,
         loadings.label = TRUE,
         loadings.colour = "black",
         loadings.label.vjust = -.5,
         loadings.label.col = "black",
         loadings.label.size = 4,
         loadings.label.font = 4) + 
  theme_bw()
```

### NMDS {.tabset}
#### Prep
1. Pick your variables
2. Switch to wide format (species listed across the top)
3. Remove any row where nothing was caught

##### Matrix
```{r matrix for nmds, warning = FALSE, message = FALSE}
Seine_f_sp <- Seine_nmds %>% dplyr::select(c("Survey", "StationCode", "Date", "FlowPulseType", "WYType",
                                    "ActionPhase", "Region", "Month", "CommonName", "CPUE", "Cond",
                                    "WTemp", "SecDepth", "Turb", "DOx"))
# Species Matrix
Seine_sp_w <- Seine_f_sp %>% pivot_wider(names_from = CommonName, values_from = CPUE, values_fill = list(CPUE=0)) %>% ungroup()

# Remove any row where there is no catch for the day.
Seine_sp_sum <- Seine_sp_w %>% mutate(Total = dplyr::select(., 14:25) %>%  rowSums(na.rm = TRUE)) %>%
  filter(Total !=0)
```

##### Transform and/or standardize for nMDS
* Some options for transformations: sqrt, log, absence/presence

```{r transform, warning = FALSE, message = FALSE}
# Sqrt transform data
sqrt.seine <- Seine_sp_sum %>% mutate_if(is.numeric, function(x) {
  sqrt(x)
})
                   
# Log transform data
log.seine <- Seine_sp_sum %>% mutate_if(is.numeric, function(x) {
  log(x + 1) })

# Absence/Presence data
bin.seine <- Seine_sp_sum %>% mutate_if(is.numeric, function(x) {
  case_when(x>0 ~0,
            x ==0 ~1)})

# Proportional data

```
 
#### Run NMDS
1. Run
* Currently a subset of data since otherwise even 2000 iterations won't converge :(
* Went with sqrt transformation - least change

```{r seine nmds, warning = FALSE, message = FALSE, results = FALSE}
library(vegan)

test.seine <- sample_n(sqrt.seine, 120)
str(test.seine)

# NMDS Seine
# This can be slow
seine.nmds <- metaMDS(test.seine[,14:25], distance="bray", k=2, trymax=500, autotransform = FALSE)
seine.nmds

```

2. Check NMDS
#### Interpretation
* Stress < 0.15 is acceptable fit.
* Check stressplot.
```{r nmdsInterp, message = FALSE, warning = FALSE}
# Scree Plot - what should k=? 
# This can be slow
nmds.scree(test.seine[,14:25], distance='bray', k=5, trymax = 300, autotransform = FALSE) 

### Check NMDS solution 
# Large scatter is not good
stressplot(seine.nmds)


```


#### Plots!
3. Plot Results. 
* Resources:
- https://jonlefcheck.net/2012/10/24/nmds-tutorial-in-r/
- https://rpubs.com/collnell/manova
- https://chrischizinski.github.io/rstats/vegan-ggplot2/
```{r nmdsplot, message = FALSE, warning = FALSE}
### NMDS scores
seine.scores <- as.data.frame(scores(seine.nmds))

# Need a category to merge scores with the rest of env data
row.names(seine.scores) <- row.names(test.seine)
seine.nmdsplot <- cbind(seine.scores, test.seine)


# Make Plot
plot(seine.nmds)
ordiplot(seine.nmds, type = "n")
orditorp(seine.nmds, display = "species", col = "red", air = 0.01)
orditorp(seine.nmds, display = "sites", cex = 1, air = 0.01)

species.scores <- as.data.frame(scores(seine.nmds, "species"))
species.scores$species <- rownames(species.scores)
head(species.scores)

# Prettier Plots
# Option 1
ggplot(seine.nmdsplot, aes(NMDS1, NMDS2, color = ActionPhase)) + 
  geom_point(position = position_jitter(.1)) +
  geom_text(aes(label = WYType)) +
  stat_ellipse(type = 't', size = 1) +
  annotate("text", x = min(seine.nmdsplot$NMDS1), y = min(seine.nmdsplot$NMDS2), label = paste('Stress = ', round(seine.nmds$stress,3))) + # Add stress to plot
  theme_minimal() 

# Option 2
# With species scores and boxes

# Make the datasets for the boxes
grp.a <- seine.nmdsplot[seine.nmdsplot$ActionPhase == "Pre", ][chull(seine.nmdsplot[seine.nmdsplot$ActionPhase == 
    "Pre", c("NMDS1", "NMDS2")]), ]  # hull values for grp A
grp.b <- seine.nmdsplot[seine.nmdsplot$ActionPhase == "During", ][chull(seine.nmdsplot[seine.nmdsplot$ActionPhase == 
    "During", c("NMDS1", "NMDS2")]), ]  # hull values for grp B
grp.c<- seine.nmdsplot[seine.nmdsplot$ActionPhase == "Post", ][chull(seine.nmdsplot[seine.nmdsplot$ActionPhase == 
    "Post", c("NMDS1", "NMDS2")]), ]  # hull values for grp C
hull.data0 <- rbind(grp.a, grp.b)  #combine grp.a and grp.b
hull.data <- rbind(hull.data0,grp.c)

# Plot
ggplot() + 
  geom_point(data = seine.nmdsplot, aes(x = NMDS1, y = NMDS2, color = ActionPhase),
             position = position_jitter(.1)) +
  geom_text(data = seine.nmdsplot, aes(NMDS1, NMDS2, label = WYType, color = ActionPhase)) +
  geom_text(data = species.scores, aes(x = NMDS1, y = NMDS2, label= species), color = "black", size = 5) +
  theme_minimal() +
  annotate("text", x = min(seine.nmdsplot$NMDS1), y = min(seine.nmdsplot$NMDS2), label = paste('Stress = ', round(seine.nmds$stress,3))) + # Add stress to plot
  geom_polygon(data=hull.data,aes(x=NMDS1,y=NMDS2,fill=ActionPhase,group=ActionPhase),alpha=0.30) 

```

### PERMANOVA  {.tabset}

#### Run Models
* You can include both continuous and categorical variables
1. Transform continuous variables
2. Standardize species data (basically becomes proportional data) to reduce influence of large numbers. Could also do sqrt transformation.
3. Run PERMANOVA with each variable separately. Only keep significant variables.
4. Create model that is significant with greatest R2, move down the list until new variables are not signficant.
5. For any categorical variables, test interactions as well.

```{r permanova, warning = FALSE, message = FALSE}
# Row standardization
seine.st <- data.stand(Seine_sp_sum[,14:25], method = 'total', margin = 'row', plot = F)

# Individual permanova models
(perm.WY <- adonis(formula = seine.st~WYType, data = Seine_sp_sum, method = "bray", permutations = 99))
(perm.Action <- adonis(formula = seine.st~ActionPhase, data = Seine_sp_sum, method = "bray", permutations = 99))
(perm.FlowPulse <- adonis(formula = seine.st~FlowPulseType, data = Seine_sp_sum, method = "bray", permutations = 99))
(perm.Month <- adonis(formula = seine.st~Month, data = Seine_sp_sum, method = "bray", permutations = 99))
(perm.Survey <- adonis(formula = seine.st~Region, data = Seine_sp_sum, method = "bray", permutations = 99))

perm.2 <- adonis(formula = seine.st~Region * Month, data = Seine_sp_sum, method = "bray", 
                 permutations = 99)
perm.2

```

#### Homogeneity of Variance
6. Test for Homogeneity of Variance
* Choose variables in your model
* You want the anova not to be significant (significant means variances are not homogeneous)
* Boxplots should be pretty equal
```{r betadispersion, warning = FALSE, message = FALSE}
# Make dissimilarity matrix
spe.d <- vegdist(seine.st, "bray")

# StationCode
(sp.bdp <- betadisper(spe.d, Seine_sp_sum$Region))
anova(sp.bdp)
permutest(sp.bdp,pairwise=TRUE)
plot(sp.bdp, ellipse = TRUE)
boxplot(sp.bdp)

# Month
(sp.bdp2 <- betadisper(spe.d, Seine_sp_sum$Month))
anova(sp.bdp2)
permutest(sp.bdp2,pairwise=TRUE)
plot(sp.bdp2, ellipse = TRUE)
boxplot(sp.bdp2)

# WYType
sp.bdp3 <- betadisper(spe.d, Seine_sp_sum$WYType)
sp.bdp3
anova(sp.bdp3)
permutest(sp.bdp3,pairwise=TRUE)
plot(sp.bdp3, ellipse = TRUE)
boxplot(sp.bdp3)

# ActionPhase
sp.bdp4 <- betadisper(spe.d, Seine_sp_sum$ActionPhase)
sp.bdp4
anova(sp.bdp4)
permutest(sp.bdp4,pairwise=TRUE)
plot(sp.bdp4, ellipse = TRUE)
boxplot(sp.bdp4)

# FlowPulseType
sp.bdp5 <- betadisper(spe.d, Seine_sp_sum$FlowPulseType)
sp.bdp5
anova(sp.bdp5)
permutest(sp.bdp5,pairwise=TRUE)
plot(sp.bdp5, ellipse = TRUE)
boxplot(sp.bdp5)

```

### CCA {.tabset}
#### Run CCA
* Format: 1 matrix of predictors, 1 matrix of species data
1. Transform and standardize continuous variables
2. Transform species data - sqrt
3. Check if unimodal distribution 
```{r cca, warning = FALSE, message = FALSE}

# Prepare continuous data
seine.env.scaled <- as.data.frame(scale(Seine_sp_sum[,9:13]),center = "TRUE", scale = "TRUE")
seine.env.f <- cbind(seine.env.scaled, Seine_sp_sum[,c("WYType", "ActionPhase", "FlowPulseType", "Month", "Region")])

# Prepare species data
seine.sp <- sqrt.seine[,14:25]

# Check unimodal distribution
# Axis length > 4 = unimodal (CCA)
# 2-4 probably unimodal
# <2 = linear model
decorana(seine.sp, ira=0)

# Run CCA
spe.cca <- cca(seine.sp ~.,seine.env.f)
summary(spe.cca)
```

#### Interpretation
4. Interpretation of CCA axis, terms
* global model
* axis
* terms (order matters)
* Can use a step  model to create final CCA model
```{r cca.interp, message = FALSE, warning = FALSE}

# Test the significance of the CCA
anova(spe.cca)
anova(spe.cca, by = "axis")
anova(spe.cca, by = "terms")
anova(spe.cca, by = 'margin')

# Forward stepping
cca.step <- ordistep(cca(seine.sp ~ 1, data = seine.env.f), scope = formula(spe.cca), 
                        direction = "forward", pstep = 1000)
```

#### Plot
5. Plot CCA
```{r ccaplot, message = FALSE, warning = FALSE}


plot(spe.cca, choices = c(1,2), display = c('wa', 'sp', 'bp'), scaling = 2)
# Plot the CCA

cca.biplot = function(cca){
  
  #find plot dimensions (changed these so all the points would fit)
  xmin = min(summary((cca))$species[, 1]) * 1.6
  xmax = max(summary((cca))$species[, 1]) * 1.4
  ymin = min(summary((cca))$species[, 2]) * 1.2
  ymax = max(summary((cca))$species[, 2]) * 2.2
  
  par(bty='l')
  
  # plot(cca,disp='species',scaling=1) #scale to show species the best
  plot(summary((cca))$species[, 1], summary((cca))$species[, 2],
       type = 'n', xlim = c(xmin, xmax), ylim = c(ymin, ymax), 
       ylab = 'CA2 (8.0% Variation Explained)', xlab = 'CA1 (3.5% Variation Explained)')
  axis(side = 1, lwd = 2)
  axis(side = 2, lwd = 2)
  box(lwd = 2)
  
  #draw origin lines
  segments(-2, 0, 2, 0, lwd = 1, lty = 3)
  segments(0, -4, 0, 2, lwd = 1, lty = 3)
  
  #Add species names
  text(summary((cca))$species[, 1],summary((cca))$species[, 2], labels = rownames(summary((cca))$species), cex = 0.8)
  
  #define continuous variables possibly used
  cont.vars=c('Cond','WTemp','DOx','SecDepth','Turb')
  
  bi.names = row.names(summary(cca)$biplot) #names of environmental variables
  # centroid.names=row.names(summary(cca)$centroid) 
  centroid.names = data.frame(Rows = rownames(summary((cca))$centroid))
  
  #Pain in the a$$ way to do this, but I created abbreviated names for each level of
  #my categorical variable so that they would look nicer when plotted
  #Else I would have names like HabitatC on the biplot
  # 
  centroid.names$New[centroid.names$Rows == 'ActionPhasePre'] = 'PreAction'
  centroid.names$New[centroid.names$Rows == 'ActionPhaseDuring'] = 'DuringAction'
  centroid.names$New[centroid.names$Rows == 'ActionPhasePost'] = 'PostAction'

  centroid.names$New[centroid.names$Rows == 'WYTypeC'] = 'CriticalWY'
  centroid.names$New[centroid.names$Rows == 'WYTypeD'] = 'DryWY'
  centroid.names$New[centroid.names$Rows == 'WYTypeW'] = 'WetWY'

  centroid.names$New[centroid.names$Rows == 'FlowPulseTypeMA-Ag'] = 'AgPulse'
  centroid.names$New[centroid.names$Rows == 'FlowPulseTypeMA-SR'] = 'SRPulse'
  centroid.names$New[centroid.names$Rows == 'FlowPulseTypeNF'] = 'NoFA'
  
  centroid.names$New[centroid.names$Rows == 'Month4'] = 'Apr'
  centroid.names$New[centroid.names$Rows == 'Month5'] = 'May'
  centroid.names$New[centroid.names$Rows == 'Month6'] = 'June'
  centroid.names$New[centroid.names$Rows == 'Month7'] = 'July'
  centroid.names$New[centroid.names$Rows == 'Month8'] = 'Aug'
  centroid.names$New[centroid.names$Rows == 'Month9'] = 'Sep'
  centroid.names$New[centroid.names$Rows == 'Month10'] = 'Oct'
  centroid.names$New[centroid.names$Rows == 'Month11'] = 'Nov'
  centroid.names$New[centroid.names$Rows == 'Month12'] = 'Dec'
  centroid.names$New[centroid.names$Rows == 'Month1'] = 'Jan'
  centroid.names$New[centroid.names$Rows == 'Month2'] = 'Feb'
  centroid.names$New[centroid.names$Rows == 'Month3'] = 'Mar'
  
  
  #This could possibly be slimmed down, but it plots arrows if the variable is continuous (part of the list above)
  # and plots a point for the categorical variables instead
  
  for(i in 1:length(summary(cca)$biplot[, 1])){
    
    #Test that the row name is one of the continuous variables before plotting arrows
    if(bi.names[i] %in% cont.vars){
      arrows(0, 0, summary(cca)$biplot[i, 1], summary(cca)$biplot[i, 2],
             lwd = 1, angle = 25, length = 0.10, col = "#F4aa42")
      text(summary(cca)$biplot[i, 1] * 1.5, 
           summary(cca)$biplot[i, 2] * 1.5, labels = bi.names[i], col = '#F4aa42', cex = 0.8, font = 4)
    }
    
  }
  
  #now plot the centered mean value of each nominal variable
  #Use if statement to test for case where I don't have any categorical variables
  
  if(is.numeric(summary(cca)$centroids) == TRUE){
    
    points(summary(cca)$centroids[, 1] * 0.9, summary(cca)$centroids[, 2] * 0.9, pch = 15, col = 'black')
    
    text(summary(cca)$centroids[, 1] * 0.9, (summary(cca)$centroids[, 2] * 0.9 + 0.15), labels = centroid.names$New,
         col = 'darkcyan', cex = 0.7, adj = .5)
  }
  
}

cca.biplot(spe.cca)



```
